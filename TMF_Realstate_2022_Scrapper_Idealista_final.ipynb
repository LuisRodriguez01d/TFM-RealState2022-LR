{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TMF-Realstate-2022-Scrapper-Idealista-final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFoEbJD3VZNcxQxkRpVaV6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisRodriguez01d/TFM-RealState2022-LR/blob/main/TMF_Realstate_2022_Scrapper_Idealista_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if scrapy no installed\n",
        "!pip install scrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDiUT1OuucSX",
        "outputId": "65d7694d-172e-4a27-bf03-8bb4355c8f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.6.1-py2.py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting w3lib>=1.17.0\n",
            "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting queuelib>=1.4.2\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting protego>=0.1.15\n",
            "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting zope.interface>=4.1.3\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 44.8 MB/s \n",
            "\u001b[?25hCollecting parsel>=1.5.0\n",
            "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting cryptography>=2.0\n",
            "  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 44.1 MB/s \n",
            "\u001b[?25hCollecting tldextract\n",
            "  Downloading tldextract-3.2.0-py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting itemloaders>=1.0.1\n",
            "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
            "Collecting itemadapter>=0.1.0\n",
            "  Downloading itemadapter-0.5.0-py3-none-any.whl (10 kB)\n",
            "Collecting pyOpenSSL>=16.2.0\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy) (57.4.0)\n",
            "Collecting PyDispatcher>=2.0.5\n",
            "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.1\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Twisted>=17.9.0\n",
            "  Downloading Twisted-22.2.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 38.1 MB/s \n",
            "\u001b[?25hCollecting service-identity>=16.0.0\n",
            "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.2.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n",
            "Collecting jmespath>=0.9.5\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (21.4.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy) (3.10.0.2)\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting constantly>=15.1\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting Automat>=0.8.0\n",
            "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting incremental>=21.3.0\n",
            "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (2.23.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n",
            "Building wheels for collected packages: PyDispatcher\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11516 sha256=111250cb4dc69d90729c0e6ee7791255fe9936912eef6707becbe0151ba0c0ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/18/21/3c6a732eaa69a339198e08bb63b7da2c45933a3428b29ec454\n",
            "Successfully built PyDispatcher\n",
            "Installing collected packages: w3lib, cssselect, zope.interface, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, cryptography, constantly, Automat, Twisted, tldextract, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-22.2.0 constantly-15.1.0 cryptography-36.0.2 cssselect-1.1.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.5.0 itemloaders-1.0.4 jmespath-1.0.0 parsel-1.6.0 protego-0.2.1 pyOpenSSL-22.0.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.6.1 service-identity-21.1.0 tldextract-3.2.0 w3lib-1.22.0 zope.interface-5.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfwe5BHE5gul"
      },
      "outputs": [],
      "source": [
        "# Packages\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.selector import Selector\n",
        "import json\n",
        "import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load postcodes.txt\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "9ozp2JYzsAfk",
        "outputId": "c6a62977-baa1-409a-e27b-9399e7a6a37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5edf07e1-8daf-4887-964d-f2c165fd965c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5edf07e1-8daf-4887-964d-f2c165fd965c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving postcodes.txt to postcodes.txt\n",
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Mar 20 14:26 .\n",
            "drwxr-xr-x 1 root root 4096 Mar 20 11:58 ..\n",
            "drwxr-xr-x 1 root root 4096 Mar  9 14:47 .config\n",
            "-rw-r--r-- 1 root root  390 Mar 20 14:26 postcodes.txt\n",
            "drwxr-xr-x 1 root root 4096 Mar  9 14:48 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# idealista scraper class\n",
        "class CommercialSale(scrapy.Spider):\n",
        "    #scraper name\n",
        "    name = 'idealist'\n",
        "\n",
        "    # base URL\n",
        "    base_url = 'https://www.idealista.com/buscar/venta-viviendas/'\n",
        "    # custom headers\n",
        "    headers = {\n",
        "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "        'accept-encoding': 'gzip, deflate, br',\n",
        "        'accept-language': 'es-ES,es;q=0.9',\n",
        "        'cache-control': 'max-age=0',\n",
        "        #'cookie': 'userUUID=3cd82b75-5416-43da-93c3-f29834bf2c58; contactf429c567-b38f-4aca-982d-01bcbf88d0c8=\"{'email':null,'phone':null,'phonePrefix':null,'friendEmails':null,'name':null,'message':null,'message2Friends':null,'maxNumberContactsAllow':10,'defaultMessage':true}\"; SESSION=aef06c73bc8de1dd~f429c567-b38f-4aca-982d-01bcbf88d0c8; utag_main=v_id:017fa60afed00011a6e36c0cfd5905072003706a0086e$_sn:1$_se:1$_ss:1$_st:1647760012818$ses_id:1647758212818%3Bexp-session$_pn:1%3Bexp-session$_prevVtSource:directTraffic%3Bexp-1647761813195$_prevVtCampaignCode:%3Bexp-1647761813195$_prevVtDomainReferrer:%3Bexp-1647761813195$_prevVtSubdomaninReferrer:%3Bexp-1647761813195$_prevVtUrlReferrer:%3Bexp-1647761813195$_prevVtCampaignLinkName:%3Bexp-1647761813195$_prevVtCampaignName:%3Bexp-1647761813195$_prevVtRecommendationId:%3Bexp-1647761813195$_prevCompletePageName:16%3A%3Alisting%3A%3AresultList%3A%3Aothers%3A%3Ahttps%3A%2F%2Fwww.idealista.com%2Fbuscar%2Fventa-viviendas%2F28001%2F%3Bexp-1647761813196$_prevLevel2:16%3Bexp-1647761813196$_prevCompleteClickName:; atuserid=%7B%22name%22%3A%22atuserid%22%2C%22val%22%3A%221486e12f-d085-484d-9ab1-63c2ff0b1b38%22%2C%22options%22%3A%7B%22end%22%3A%222023-04-21T06%3A36%3A53.354Z%22%2C%22path%22%3A%22%2F%22%7D%7D; atidvisitor=%7B%22name%22%3A%22atidvisitor%22%2C%22val%22%3A%7B%22vrn%22%3A%22-582065-%22%7D%2C%22options%22%3A%7B%22path%22%3A%22%2F%22%2C%22session%22%3A15724800%2C%22end%22%3A15724800%7D%7D; datadome=R.d8XtdQpWDBw8vjRBrqqYh85nY9PIqY3LF.L5X6JVGKse.q_LClvDTEKQlP4udr3Sf-A1VzoDJb_movar0aUhcOurHLc2V01pPt49yGp87rgtQ6Bf0YmaB0-ujgLaw; didomi_token=eyJ1c2VyX2lkIjoiMTdmYTYwYWYtZjFiMS02ZTYyLWI0MzItNDIzYjcwOTE1ZmI0IiwiY3JlYXRlZCI6IjIwMjItMDMtMjBUMDY6Mzc6MjQuMjY5WiIsInVwZGF0ZWQiOiIyMDIyLTAzLTIwVDA2OjM3OjI0LjI2OVoiLCJ2ZW5kb3JzIjp7ImVuYWJsZWQiOlsiZ29vZ2xlIiwiYzptaXhwYW5lbCIsImM6YWJ0YXN0eS1MTGtFQ0NqOCIsImM6aG90amFyIiwiYzp5YW5kZXhtZXRyaWNzIiwiYzpiZWFtZXItSDd0cjdIaXgiLCJjOmFwcHNmbHllci1HVVZQTHBZWSIsImM6dGVhbGl1bWNvLURWRENkOFpQIiwiYzppZGVhbGlzdGEtTHp0QmVxRTMiLCJjOmlkZWFsaXN0YS1mZVJFamUyYyJdfSwicHVycG9zZXMiOnsiZW5hYmxlZCI6WyJhbmFseXRpY3MtSHBCSnJySzciLCJnZW9sb2NhdGlvbl9kYXRhIl19LCJ2ZXJzaW9uIjoyLCJhYyI6IkFGbUFDQUZrLkFBQUEifQ==; euconsent-v2=CPWIFEAPWIFEAAHABBENCECoAP_AAAAAAAAAF5wBAAIAAtAC2AvMAAABAaADAAEESyUAGAAIIllIAMAAQRLIQAYAAgiWOgAwABBEsJABgACCJYyADAAEESxUAGAAIIlg.f_gAAAAAAAAA; ABTasty=uid=na0xhbr3s42pd7h5&fst=1647758244682&pst=-1&cst=1647758244682&ns=1&pvt=1&pvis=1&th=; ABTastySession=mrasn=&sen=0&lp=https%253A%252F%252Fwww.idealista.com%252Fbuscar%252Fventa-viviendas%252F28001%252F; TestIfCookie=ok; TestIfCookieP=ok; pbw=%24b%3d16990%3b%24o%3d11100%3b%24sw%3d1280%3b%24sh%3d768; vs=33114=4845997; pid=3849401358877207874; sasd2=q=%24qc%3D1314173456%3B%24ql%3DMedium%3B%24qpc%3D47014%3B%24qt%3D228_3343_87100t%3B%24dma%3D0&c=1&l=-231065759&lo=-2146930231&lt=637833586499569603&o=1; sasd=%24qc%3D1314173456%3B%24ql%3DMedium%3B%24qpc%3D47014%3B%24qt%3D228_3343_87100t%3B%24dma%3D0',\n",
        "        #'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"99\", \"Google Chrome\";v=\"99\"',\n",
        "        #'sec-ch-ua-mobile': '?0'\n",
        "        #'sec-ch-ua-platform': \"Windows\",\n",
        "        'sec-fetch-dest': 'document',\n",
        "        'sec-fetch-mode': 'navigate',\n",
        "        'sec-fetch-site': 'none',\n",
        "        'sec-fetch-user': '?1',\n",
        "        'upgrade-insecure-requests': '1',\n",
        "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    # customer settings\n",
        "    custom_settings = {\n",
        "        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,\n",
        "        'DOWNLOAD_DELAY': 1\n",
        "    }\n",
        "\n",
        "    # current page counter\n",
        "    current_page = 1\n",
        "\n",
        "    # postcodes list\n",
        "    postcodes = []\n",
        "\n",
        "    # init constructor\n",
        "    def __init__(self):\n",
        "      #postcodes content\n",
        "      content = ''\n",
        "\n",
        "      #open postcodes.txt\" file\n",
        "      with open('postcodes.txt', 'r') as f:\n",
        "        for line in f.read():\n",
        "            content += line\n",
        "      \n",
        "      # parse content\n",
        "      self.postcodes = list(filter(None, content.split('\\n')))\n",
        "      \n",
        "    # crawler´s entry\n",
        "    def start_requests(self):\n",
        "        #init filename\n",
        "        filename = './content/sale_' + datetime.datetime.today().strftime('%Y-%m-%d-%H-%M') + '.jsonl'\n",
        "        \n",
        "        #reset current page counter\n",
        "        self.current_page = 1\n",
        "\n",
        "        # postcode counter\n",
        "        count = 1\n",
        "\n",
        "        #loop over  post codes\n",
        "        for postcode in self.postcodes:\n",
        "          # generate next postcode URL\n",
        "          next_postcode = self.base_url + str(postcode) + '/'\n",
        "\n",
        "          # crawl next postcode URL\n",
        "          yield scrapy.Request(\n",
        "              url=next_postcode,\n",
        "              headers=self.headers,\n",
        "              meta={\n",
        "                  'postcode': postcode,\n",
        "                  'filename': filename,\n",
        "                  'count': count\n",
        "              },\n",
        "              callback=self.parse_links\n",
        "          )\n",
        "          #incrememnt curren postcode counter\n",
        "          count += 1\n",
        "          #break\n",
        "\n",
        "\n",
        "    # parse link\n",
        "    def parse_links(self, response):\n",
        "        # extract meta data\n",
        "        postcode = response.meta.get('postcode')\n",
        "        filename = response.meta.get('filename')\n",
        "        count = response.meta.get('count')\n",
        "       \n",
        "        # cards URLs\n",
        "        card_urls =[]\n",
        "\n",
        "        # entract hightop cards\n",
        "        cards_hightop = response.css('article[class=\"item   item_contains_branding item_hightop item-multimedia-container\"]')\n",
        "        cards_casual = response.css('article[class=\"item   item_contains_branding item-multimedia-container\"]')\n",
        "\n",
        "        #extract hightop card URLs\n",
        "        for card in cards_hightop:\n",
        "          card_urls.append(card.css('a[class=\"item-link \"]::attr(href)').get())\n",
        "\n",
        "        #extract casual card URLs\n",
        "        for card in cards_casual:\n",
        "          card_urls.append(card.css('a[class=\"item-link \"]::attr(href)').get())\n",
        "        \n",
        "        # loop over property card URLs\n",
        "        for card_url in card_urls:\n",
        "                 \n",
        "         \n",
        "          #crawl property listing\n",
        "          yield response.follow(\n",
        "              url=card_url,\n",
        "              headers=self.headers,\n",
        "              meta={\n",
        "                  'postcode': postcode,\n",
        "                  'filename': filename\n",
        "              },\n",
        "              callback=self.parse_listing\n",
        "          )\n",
        "          #break\n",
        "          \n",
        "\n",
        "          \n",
        "        # handle pagination within each postcode url\n",
        "        try:\n",
        "          try:\n",
        "            #extrat total pages\n",
        "            total_pages = response.css('div[class=\"pagination\"]')\n",
        "            total_pages = total_pages.css('li *::text').getall()\n",
        "            total_pages = max([\n",
        "                               int(page)\n",
        "                               for page in total_pages\n",
        "                               if page.isdigit()\n",
        "                               ])\n",
        "            \n",
        "            #increment current page counter\n",
        "            self.current_page += 1\n",
        "\n",
        "          except:\n",
        "            total_pages = 1\n",
        "            self.current_page = 1\n",
        "          \n",
        "          # print debug info\n",
        "          print('POSTCODE %s | %s out of %s postcodes' % (postcode, count, len(self.postcodes)))\n",
        "          \n",
        "          # check the if current page is within the legal page range\n",
        "          if self.current_page <= total_pages:\n",
        "            #generate next page url\n",
        "            next_page = self.base_url + str(postcode) + '/'\n",
        "            next_page += 'pagina-' + str(self.current_page) + '.htm'\n",
        "            \n",
        "\n",
        "            #print debug information\n",
        "            print('PAGE %s | %s' % (self.current_page, total_pages))\n",
        "\n",
        "            \n",
        "            # crawl next page\n",
        "            yield response.follow(\n",
        "                meta={\n",
        "                    'postcode': postcode,\n",
        "                    'filename': filename,\n",
        "                    'count': count\n",
        "                },\n",
        "                callback=self.parse_links\n",
        "            )\n",
        "            \n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "          pass\n",
        "          \n",
        "\n",
        "\n",
        "     # parse property card listing\n",
        "    def parse_listing(self, response):\n",
        "        #extract meta data\n",
        "        postcode = response.meta.get('postcode')\n",
        "        filename = response.meta.get('filename')\n",
        "         \n",
        "\n",
        "        #data extraction logic\n",
        "        features = {\n",
        "            'id': response.url.split('/?')[0].split('/')[-1],\n",
        "            'url': response.url,\n",
        "            'postcode': postcode,\n",
        "\n",
        "            'title': response.css('span[class=\"main-info__title-main\"]::text').get(),\n",
        "\n",
        "            'address': response.css('span[class=\"main-info__title-minor\"]::text').get(),\n",
        "            #'price': response.css('span[class=\"info-data-price\"]').css('span[class=\"txt-bold\"]::text').get(),\n",
        "\n",
        "            'price': ''.join(response.css('span[class=\"info-data-price\"] *::text').getall()),\n",
        "            'price_details': list(filter(None, [\n",
        "                                            text.get().strip()\n",
        "                                            for text in\n",
        "                                            response.css('section[class=\"price-features__container\"] *::text')\n",
        "                                        ])),\n",
        "            'last_updated': '',\n",
        "\n",
        "            'floor_area': '',\n",
        "\n",
        "            'image_urls': [],\n",
        "\n",
        "            'agent_name': response.css('div[class=\"professional-name\"]')\n",
        "                                  .css('span::text')\n",
        "                                  .get()\n",
        "                                  .strip(),\n",
        "            'agent_link': 'https://www.idealista.com' +\n",
        "                          str(response.css('a[class=\"about-advertiser-name\"]::attr(href)')\n",
        "                                      .get()),\n",
        "            'agent_phone': response.css('span[class=\"phone-btn-number\"]::text')\n",
        "                                   .get()),\n",
        "\n",
        "            'full_description': response.css('div[class=\"adCommentsLanguage expandable\"] *::text').getall(),\n",
        "\n",
        "            'key_features': Selector(text=response.css('div[class=\"details-property_features\"]'))\n",
        "                                                  .css('ul')\n",
        "                                                  .css('li::text')\n",
        "                                                  .getall(),\n",
        "            'building_fabric' [],\n",
        "\n",
        "            'amenities' [],\n",
        "\n",
        "            'location': [\n",
        "                         text.get.strip()\n",
        "                         for text in\n",
        "                         response.css('div[id=\"headerMap\"]')\n",
        "                                 .css('li::text')\n",
        "                         ],\n",
        "            'coordinates': {\n",
        "                'latitude': '',|\n",
        "                'longitude': ''\n",
        "            }\n",
        "\n",
        "        }\n",
        "\n",
        "        # extract price\n",
        "        features['price'] += ' | ' + ' '.join(list(filter(None, [\n",
        "                                      text.get().strip()\n",
        "                                      for text in\n",
        "                                      Selector(text=response.css('div[class=\"info-features\"]')\n",
        "                                                            .css('span')\n",
        "                                                            .getall()[2]\n",
        "                                              ).css(' *::text')\n",
        "                                    ])))\n",
        "        \n",
        "\n",
        "        # extract building fabric\n",
        "        try:\n",
        "            features['building_fabric'] = [\n",
        "                text.strip()\n",
        "                for text in\n",
        "                Selector(\n",
        "                    text=response.css('div[class=\"details-property_features\"]')\n",
        "                                 .getall()[1]\n",
        "                ).css('li::text').getall()\n",
        "            ]\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        # extract last update date\n",
        "        try:\n",
        "          features['last_updated'] = ''.join(\n",
        "              list(filter(None, [\n",
        "                  text.get.strip()\n",
        "                  for text in\n",
        "                  response.css('div[id=\"stats\"] *::text')\n",
        "              ]))[1:-1]\n",
        "          )\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        # extract amenities\n",
        "        try:\n",
        "            features['amenities'] = [\n",
        "                text.strip()\n",
        "                for text in\n",
        "                Selector(\n",
        "                    text=response.css('div[class=\"details-property_features\"]')\n",
        "                                 .getall()[2]\n",
        "                ).css('li::text').getall()\n",
        "            ]\n",
        "        except:\n",
        "          pass\n",
        "          \n",
        "        # extract floor area\n",
        "        try:\n",
        "          features['floor_area'] = ''.join(list(filter(None, [\n",
        "                                               text.get().strip()\n",
        "                                               for text in\n",
        "                                               Selector(text=response.css('div[class=\"info-features\"]')\n",
        "                                                                     .css('span')\n",
        "                                                                     .getall()[0]\n",
        "                                                       ).css(' *::text')\n",
        "                                       ])))\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        # extract script containing JSON data\n",
        "        try:\n",
        "          script = ''.join([\n",
        "                      text.get()\n",
        "                      for text in response.css('script::text')\n",
        "                      if 'var config=' in text.get() \n",
        "          ])\n",
        "        \n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        # extract image url\n",
        "        try:\n",
        "          features['image_urls'] = [\n",
        "                                    image.split(',')[0]\n",
        "                                    for image in\n",
        "                                    script.split('imageDataService:\"')\n",
        "          ][1:]\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        # extract coordinates\n",
        "        try:\n",
        "          features['coordinates'] = {\n",
        "              latitude = script.split('latitude:\"')[1].split('\"')[0],\n",
        "              longitude = script.split('longitude:\"')[1].split('\"')[0]\n",
        "          }\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "\n",
        "        # write feature to output file\n",
        "        with open(filename, 'a') as f:\n",
        "          f.write(json.dumps(features, indent=2) + '\\n')  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#main driver\n",
        "if __name__ == '__main__':\n",
        "  # run scraper\n",
        "  process = CrawlerProcess()\n",
        "  process.crawl(CommercialSale)\n",
        "  process.start()\n",
        "\n",
        "  #CommercialSale.parse_links(CommercialSale, '')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsE72T9YAWsT",
        "outputId": "0281ae42-421b-4a29-cae2-ff801f0bb7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-20 14:33:39 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
            "2022-03-20 14:33:39 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.2.0, Python 3.7.12 (default, Jan 15 2022, 18:48:18) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1n  15 Mar 2022), cryptography 36.0.2, Platform Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-03-20 14:33:39 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1}\n",
            "2022-03-20 14:33:39 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-03-20 14:33:39 [scrapy.extensions.telnet] INFO: Telnet Password: 49bf92e95544ff7c\n",
            "2022-03-20 14:33:39 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-03-20 14:33:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-03-20 14:33:39 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-03-20 14:33:39 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-03-20 14:33:39 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-03-20 14:33:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-03-20 14:33:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-03-20 14:33:40 [filelock] DEBUG: Attempting to acquire lock 139911520191760 on /root/.cache/python-tldextract/3.7.12.final__usr__7d8fdf__tldextract-3.2.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-03-20 14:33:40 [filelock] DEBUG: Lock 139911520191760 acquired on /root/.cache/python-tldextract/3.7.12.final__usr__7d8fdf__tldextract-3.2.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-03-20 14:33:40 [filelock] DEBUG: Attempting to acquire lock 139911520193680 on /root/.cache/python-tldextract/3.7.12.final__usr__7d8fdf__tldextract-3.2.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-03-20 14:33:40 [filelock] DEBUG: Lock 139911520193680 acquired on /root/.cache/python-tldextract/3.7.12.final__usr__7d8fdf__tldextract-3.2.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-03-20 14:33:40 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): publicsuffix.org:443\n",
            "2022-03-20 14:33:41 [urllib3.connectionpool] DEBUG: https://publicsuffix.org:443 \"GET /list/public_suffix_list.dat HTTP/1.1\" 200 None\n",
            "2022-03-20 14:33:41 [filelock] DEBUG: Attempting to release lock 139911520193680 on /root/.cache/python-tldextract/3.7.12.final__usr__7d8fdf__tldextract-3.2.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-03-20 14:33:41 [filelock] DEBUG: Lock 139911520193680 released on /root/.cache/python-tldextract/3.7.12.final__usr__7d8fdf__tldextract-3.2.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-03-20 14:33:41 [filelock] DEBUG: Attempting to release lock 139911520191760 on /root/.cache/python-tldextract/3.7.12.final__usr__7d8fdf__tldextract-3.2.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-03-20 14:33:41 [filelock] DEBUG: Lock 139911520191760 released on /root/.cache/python-tldextract/3.7.12.final__usr__7d8fdf__tldextract-3.2.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-03-20 14:33:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.idealista.com/buscar/venta-viviendas/28000/> (referer: None)\n",
            "2022-03-20 14:33:41 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-03-20 14:33:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 564,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 33860,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 1.311238,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 3, 20, 14, 33, 41, 231976),\n",
            " 'httpcompression/response_bytes': 236664,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'log_count/DEBUG': 12,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 196763648,\n",
            " 'memusage/startup': 196763648,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2022, 3, 20, 14, 33, 39, 920738)}\n",
            "2022-03-20 14:33:41 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "postcodes = []\n",
        "\n",
        "    # init constructor\n",
        "\n",
        "#postcodes content\n",
        "content = ''\n",
        "#open postcodes.txt\" file\n",
        "with open('postcodes.txt', 'r') as f:\n",
        "  for line in f.read():\n",
        "    content += line\n",
        "  # parse content\n",
        "  postcodes = list(filter(None, content.split('\\n')))\n",
        "  print(postcodes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVhYVCC8OjjM",
        "outputId": "2deff8ba-26fa-4ec9-a449-f4c324cbcab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['28000', '28001', '28002', '28003', '28004', '28005', '28006', '28007', '28008', '28009', '28010', '28011', '28012', '28013', '28014', '28015', '28016', '28017', '28018', '28019', '28020', '28021', '28022', '28023', '28024', '28025', '28026', '28027', '28028', '28029', '28030', '28031', '28032', '28033', '28034', '28035', '28036', '28037', '28038', '28039', '28040', '28041', '28042', '28043', '28044', '28045', '28046', '28047', '28048', '28049', '28050', '28051', '28052', '28053', '28054', '28055']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = 'https://www.idealista.com/buscar/venta-viviendas/'\n",
        "print(base_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3jatfrcQGMW",
        "outputId": "ba31b66a-39e7-45ea-df73-6701a1c091c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.idealista.com/buscar/venta-viviendas/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for postcode in postcodes:\n",
        " # generate next postcode URL\n",
        " next_postcode = base_url + str(postcode) + '/'\n",
        " print(next_postcode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nomZe5AxUFdv",
        "outputId": "4543d42b-8c81-4649-a486-68e5c8c1b5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.idealista.com/buscar/venta-viviendas/28000/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28001/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28002/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28003/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28004/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28005/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28006/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28007/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28008/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28009/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28010/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28011/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28012/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28013/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28014/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28015/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28016/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28017/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28018/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28019/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28020/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28021/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28022/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28023/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28024/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28025/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28026/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28027/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28028/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28029/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28030/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28031/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28032/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28033/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28034/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28035/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28036/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28037/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28038/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28039/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28040/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28041/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28042/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28043/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28044/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28045/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28046/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28047/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28048/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28049/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28050/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28051/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28052/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28053/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28054/\n",
            "https://www.idealista.com/buscar/venta-viviendas/28055/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pwd\n",
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdKxYtyfrk07",
        "outputId": "582f468d-14d6-43d0-9e3b-55abe7635ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Mar 20 14:26 .\n",
            "drwxr-xr-x 1 root root 4096 Mar 20 11:58 ..\n",
            "drwxr-xr-x 1 root root 4096 Mar  9 14:47 .config\n",
            "-rw-r--r-- 1 root root  390 Mar 20 14:26 postcodes.txt\n",
            "drwxr-xr-x 1 root root 4096 Mar  9 14:48 sample_data\n"
          ]
        }
      ]
    }
  ]
}